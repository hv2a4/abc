from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time
import os
from datetime import datetime
import pytesseract
from PIL import Image
import io
import base64
import concurrent.futures
import json
import glob
import csv
from selenium.webdriver.chrome.options import Options
import chromedriver_autoinstaller
chromedriver_autoinstaller.install()
driver = webdriver.Chrome(options=chrome_options)

# Th√™m c·∫•u h√¨nh Tesseract
pytesseract.pytesseract.tesseract_cmd = "tesseract"
# pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# Thi·∫øt l·∫≠p Chrome options
chrome_options = Options()
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_experimental_option("excludeSwitches", ["enable-logging"])
chrome_options.add_argument("--headless=new")
chrome_options.add_argument("--disable-dev-shm-usage")
# driver = webdriver.Chrome(service=Service("/usr/local/bin/chromedriver"), options=chrome_options)

def get_company_links(driver, url):
    driver.get(url)
    links = []
    try:
        # Get all company elements using search-results class
        companies = WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located((By.CLASS_NAME, "search-results"))
        )
        
        for company in companies:
            # Get link from first a tag in company element
            link = company.find_element(By.TAG_NAME, "a").get_attribute('href')
            if link:
                links.append(link)
                
    except Exception as e:
        print(f"L·ªói khi l·∫•y links: {str(e)}")
    return links

def is_mobile_phone(phone):
    """Ki·ªÉm tra xem s·ªë ƒëi·ªán tho·∫°i c√≥ ph·∫£i l√† di ƒë·ªông kh√¥ng"""
    if not phone or len(phone) < 9:
        return False
    # ƒê·∫ßu s·ªë ƒëi·ªán tho·∫°i di ƒë·ªông ·ªü Vi·ªát Nam
    mobile_prefixes = ['03', '05', '07', '08', '09']
    return any(phone.startswith(prefix) for prefix in mobile_prefixes)

def get_company_detail(driver, url):
    driver.get(url)
    try:
        data = {
            'T√™n c√¥ng ty': '',
            'M√£ s·ªë thu·∫ø': '',
            'ƒê·ªãa ch·ªâ': '',
            'ƒê·∫°i di·ªán ph√°p lu·∫≠t': '',
            'Tr·∫°ng th√°i': '',
            'Ng√†y c·∫•p': '',
            'ƒêi·ªán tho·∫°i': ''
        }
        
        # ƒê·ª£i v√† l·∫•y th√¥ng tin t·ª´ div class jumbotron
        company_info = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "jumbotron"))
        )
        
        # L·∫•y t√™n c√¥ng ty t·ª´ th·∫ª h4
        name_elem = company_info.find_element(By.TAG_NAME, "h4")
        data['T√™n c√¥ng ty'] = name_elem.text.strip()
        
        # L·∫•y c√°c th√¥ng tin kh√°c
        text_content = company_info.text
        for line in text_content.split('\n'):
            if 'M√£ s·ªë thu·∫ø:' in line:
                data['M√£ s·ªë thu·∫ø'] = line.replace('M√£ s·ªë thu·∫ø:', '').strip()
            elif 'ƒê·ªãa ch·ªâ:' in line:
                data['ƒê·ªãa ch·ªâ'] = line.replace('ƒê·ªãa ch·ªâ:', '').strip()
            elif 'ƒê·∫°i di·ªán ph√°p lu·∫≠t:' in line:
                data['ƒê·∫°i di·ªán ph√°p lu·∫≠t'] = line.replace('ƒê·∫°i di·ªán ph√°p lu·∫≠t:', '').strip()
            elif 'Tr·∫°ng th√°i:' in line:
                data['Tr·∫°ng th√°i'] = line.replace('Tr·∫°ng th√°i:', '').strip()
            elif 'Ng√†y c·∫•p gi·∫•y ph√©p:' in line:
                data['Ng√†y c·∫•p'] = line.replace('Ng√†y c·∫•p gi·∫•y ph√©p:', '').strip()
            elif 'ƒêi·ªán tho·∫°i tr·ª• s·ªü:' in line:
                phone = line.replace('ƒêi·ªán tho·∫°i tr·ª• s·ªü:', '').strip()
                if is_mobile_phone(phone):
                    data['ƒêi·ªán tho·∫°i'] = phone
        
        # X·ª≠ l√Ω OCR s·ªë ƒëi·ªán tho·∫°i
        try:
            phone_img = company_info.find_element(By.XPATH, ".//text()[contains(.,'ƒêi·ªán tho·∫°i tr·ª• s·ªü:')]/following-sibling::img")
            if phone_img:
                img_src = phone_img.get_attribute('src')
                if 'base64,' in img_src:
                    img_data = base64.b64decode(img_src.split(',')[1])
                    image = Image.open(io.BytesIO(img_data))
                    
                    text = pytesseract.image_to_string(
                        image,
                        config='--psm 7 -c tessedit_char_whitelist=0123456789'
                    )
                    
                    phone = ''.join(filter(str.isdigit, text))
                    if is_mobile_phone(phone):
                        data['ƒêi·ªán tho·∫°i'] = phone
                        print(f"ƒê√£ t√¨m th·∫•y s·ªë ƒëi·ªán tho·∫°i di ƒë·ªông qua OCR: {phone}")
                        return data

        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω ·∫£nh s·ªë ƒëi·ªán tho·∫°i: {str(e)}")

        return data

    except Exception as e:
        print(f"L·ªói khi l·∫•y chi ti·∫øt c√¥ng ty {url}: {str(e)}")
        return None

def save_to_excel(companies, area_name, page_num=None):
    # T·∫°o th∆∞ m·ª•c data/ho-chi-minh/{area_name} n·∫øu ch∆∞a c√≥
    data_dir = f"data/ho-chi-minh/{area_name}"
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
    
    # T·∫°o t√™n file v·ªõi timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if page_num:
        filename = f"{data_dir}/companies_page{page_num}_{timestamp}.xlsx"
    else:
        filename = f"{data_dir}/companies_full_{timestamp}.xlsx"
    
    df = pd.DataFrame(companies)
    df.to_excel(filename, index=False)
    print(f"ƒê√£ l∆∞u d·ªØ li·ªáu v√†o file: {filename}")
    return filename

def crawl_area(driver, area_path):
    base_url = f"https://www.tratencongty.com/thanh-pho-ho-chi-minh/{area_path}"
    # ƒê·ªçc checkpoint
    start_page = get_start_page_from_files(area_path)
    print(f"B·∫Øt ƒë·∫ßu crawl {area_path} t·ª´ trang {start_page}")

    page = start_page
    all_companies = []

    while True:
        current_url = f"{base_url}/?page={page}" if page > 1 else base_url
        print(f"ƒêang x·ª≠ l√Ω trang {page}...")

        company_links = get_company_links(driver, current_url)
        if not company_links:
            print(f"Kh√¥ng t√¨m th·∫•y c√¥ng ty n√†o ·ªü trang {page}")
            break

        print(f"T√¨m th·∫•y {len(company_links)} c√¥ng ty trong trang {page}")
        for link in company_links:
            print(f"ƒêang x·ª≠ l√Ω: {link}")
            company_detail = get_company_detail(driver, link)
            if company_detail:
                company_detail['Khu v·ª±c'] = area_path
                saver.save_company(company_detail)  # üîπ L∆∞u ngay v√†o CSV
                all_companies.append(company_detail)
                print("ƒê√£ l∆∞u c√¥ng ty v√†o CSV t·∫°m!")
            time.sleep(1.5)  # C√≥ th·ªÉ gi·∫£m xu·ªëng v√¨ ƒë√£ l∆∞u incremental

        # Xong 1 trang ‚Üí export CSV sang Excel
        saver.export_to_excel(page)
        save_checkpoint(area_path, page)

        page += 1
        time.sleep(0.3)

    return all_companies

def save_checkpoint(area_name, page):
    os.makedirs("checkpoints", exist_ok=True)
    ckpt_file = f"checkpoints/{area_name.replace('/', '_')}.json"
    with open(ckpt_file, "w", encoding="utf-8") as f:
        json.dump({"last_page": page}, f)
    print(f"ƒê√£ l∆∞u checkpoint: {area_name} - Trang {page}")

def load_checkpoint(area_name):
    ckpt_file = f"checkpoints/{area_name.replace('/', '_')}.json"
    if os.path.exists(ckpt_file):
        with open(ckpt_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data.get("last_page", 1)
    return 1

def get_start_page_from_files(area_path):
    # Th∆∞ m·ª•c ch·ª©a file excel
    data_dir = f"data/ho-chi-minh/{area_path.replace('/', '_')}"
    if not os.path.exists(data_dir):
        return 1  # N·∫øu ch∆∞a c√≥ th∆∞ m·ª•c th√¨ b·∫Øt ƒë·∫ßu t·ª´ trang 1

    # L·∫•y danh s√°ch t·∫•t c·∫£ file Excel trong th∆∞ m·ª•c
    files = glob.glob(f"{data_dir}/*.xlsx")

    if not files:
        return 1  # N·∫øu ch∆∞a c√≥ file th√¨ b·∫Øt ƒë·∫ßu t·ª´ trang 1

    # S·ªë file hi·ªán t·∫°i = s·ªë trang ƒë√£ crawl
    return len(files) + 1  # Trang k·∫ø ti·∫øp


def crawl_area(driver, area_path):
    base_url = f"https://www.tratencongty.com/thanh-pho-ho-chi-minh/{area_path}"
    all_companies = []

    # ƒê·ªçc checkpoint
    start_page = get_start_page_from_files(area_path)
    print(f"B·∫Øt ƒë·∫ßu crawl {area_path} t·ª´ trang {start_page}")

    page = start_page
    while True:
        current_url = f"{base_url}/?page={page}" if page > 1 else base_url
        print(f"ƒêang x·ª≠ l√Ω trang {page}...")

        page_companies = []
        company_links = get_company_links(driver, current_url)
        if not company_links:
            print(f"Kh√¥ng t√¨m th·∫•y c√¥ng ty n√†o ·ªü trang {page}")
            break

        print(f"T√¨m th·∫•y {len(company_links)} c√¥ng ty trong trang {page}")
        for link in company_links:
            print(f"ƒêang x·ª≠ l√Ω: {link}")
            company_detail = get_company_detail(driver, link)
            if company_detail:
                company_detail['Khu v·ª±c'] = area_path
                page_companies.append(company_detail)
                all_companies.append(company_detail)
                print("ƒê√£ l·∫•y th√¥ng tin th√†nh c√¥ng!")
            time.sleep(2)

        if page_companies:
            save_to_excel(page_companies, area_path.replace('/', '_'), page)
            save_checkpoint(area_path, page)  # üîπ L∆∞u checkpoint sau khi xong 1 trang

        page += 1
        time.sleep(1)

    return all_companies


try:
    driver = webdriver.Chrome(options=chrome_options)

    start_time = time.time()  # Th·ªùi gian b·∫Øt ƒë·∫ßu to√†n b·ªô qu√° tr√¨nh

    # Danh s√°ch c√°c khu v·ª±c c·ªßa TP HCM, ∆∞u ti√™n Qu·∫≠n 1
    areas_hcm = [
        # "quan-tan-binh",
        # "quan-tan-phu",
        "quan-1",  # ∆Øu ti√™n c√†o tr∆∞·ªõc
        # "quan-3",
        "quan-binh-thanh",
        # "quan-phu-nhuan",
        "quan-go-vap",
        # "quan-7",
        "quan-10",
        # "quan-5",
        "quan-6",
        # "quan-8",
        "quan-4",
        # "quan-2",
        "quan-9",
        # "quan-12",
        "huyen-binh-chanh",
        # "huyen-hoc-mon",
        "huyen-cu-chi",
        # "huyen-nha-be",
        "huyen-can-gio"
    ]

    all_area_companies = []


    # C√†o ri√™ng Qu·∫≠n 1 v·ªõi timeout 30 ph√∫t
    print("B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu Qu·∫≠n 1 (t·ªëi ƒëa 30 ph√∫t)...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        future_q1 = executor.submit(crawl_area, driver, areas_hcm[0])
        try:
            quan_1_companies = future_q1.result(timeout=1800)  # 30 ph√∫t
        except concurrent.futures.TimeoutError:
            print("Qu·∫≠n 1: ƒê√£ h·∫øt th·ªùi gian 30 ph√∫t!")
            quan_1_companies = []
    all_area_companies.extend(quan_1_companies)

    # L∆∞u d·ªØ li·ªáu Qu·∫≠n 1 ri√™ng
    if quan_1_companies:
        timestamp_q1 = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_file_q1 = f"data/ho-chi-minh/quan-1_{timestamp_q1}.xlsx"
        os.makedirs("data/ho-chi-minh", exist_ok=True)
        df_q1 = pd.DataFrame(quan_1_companies)
        df_q1.to_excel(final_file_q1, index=False)
        print(f"ƒê√£ l∆∞u {len(quan_1_companies)} c√¥ng ty c·ªßa Qu·∫≠n 1!")

    # C√°c khu v·ª±c c√≤n l·∫°i, m·ªói khu v·ª±c ch·∫°y ·ªü m·ªôt lu·ªìng ri√™ng bi·ªát, timeout 30 ph√∫t
    print("B·∫Øt ƒë·∫ßu c√†o c√°c khu v·ª±c c√≤n l·∫°i c·ªßa TP HCM (m·ªói khu v·ª±c t·ªëi ƒëa 30 ph√∫t)...")
    other_areas = areas_hcm[1:]
    other_results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(other_areas)) as executor:
        future_map = {executor.submit(crawl_area, driver, area): area for area in other_areas}
        for future in concurrent.futures.as_completed(future_map):
            area = future_map[future]
            try:
                area_companies = future.result(timeout=1800)  # 30 ph√∫t
            except concurrent.futures.TimeoutError:
                print(f"Khu v·ª±c {area}: ƒê√£ h·∫øt th·ªùi gian 30 ph√∫t!")
                area_companies = []
            other_results.extend(area_companies)
            # L∆∞u t·ª´ng khu v·ª±c ri√™ng
            if area_companies:
                timestamp_area = datetime.now().strftime("%Y%m%d_%H%M%S")
                final_file_area = f"data/ho-chi-minh/{area}_{timestamp_area}.xlsx"
                os.makedirs("data/ho-chi-minh", exist_ok=True)
                df_area = pd.DataFrame(area_companies)
                df_area.to_excel(final_file_area, index=False)
                print(f"ƒê√£ l∆∞u {len(area_companies)} c√¥ng ty c·ªßa khu v·ª±c {area}!")
    all_area_companies.extend(other_results)

    # L∆∞u t·ªïng h·ª£p t·∫•t c·∫£ c√°c khu v·ª±c
    if all_area_companies:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_file = f"data/ho-chi-minh/all_areas_companies_{timestamp}.xlsx"
        os.makedirs("data/ho-chi-minh", exist_ok=True)
        df = pd.DataFrame(all_area_companies)
        df.to_excel(final_file, index=False)
        print(f"ƒê√£ l∆∞u t·ªïng h·ª£p {len(all_area_companies)} c√¥ng ty c·ªßa t·∫•t c·∫£ c√°c khu v·ª±c!")

    end_time = time.time()  # Th·ªùi gian k·∫øt th√∫c
    elapsed = end_time - start_time  # start_time l√† th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu to√†n b·ªô qu√° tr√¨nh
    print(f"T·ªïng th·ªùi gian ho√†n th√†nh: {elapsed/60:.2f} ph√∫t ({elapsed:.1f} gi√¢y)")

except Exception as e:
    print(f"L·ªói: {str(e)}")
finally:
    # Kh√¥ng c·∫ßn driver.quit() ·ªü ƒë√¢y v√¨ m·ªói thread ƒë√£ t·ª± quit
    pass
